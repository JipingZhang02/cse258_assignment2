{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def parse(path):\n",
    "    res = list()\n",
    "    g = gzip.open(path, \"r\",encoding=\"utf-8\")\n",
    "    for l in g:\n",
    "        res.append(eval(l))\n",
    "    return res\n",
    "\n",
    "def parse_raw(path):\n",
    "    res = list()\n",
    "    with open(path,encoding=\"utf-8\") as fin:\n",
    "        lines = fin.readlines()\n",
    "        for line in tqdm(lines):\n",
    "            line = line.replace(\"true\",\"True\")\n",
    "            line = line.replace(\"false\",\"False\")\n",
    "            res.append(eval(line))\n",
    "    return res\n",
    "\n",
    "def split_raw_file(path):\n",
    "    # res = list()\n",
    "    with open(path,encoding=\"utf-8\") as fin:\n",
    "        lines = fin.readlines()\n",
    "    for i in range(10):\n",
    "        dot_pos = path.rfind(\".\")\n",
    "        save_file_path = path[:dot_pos]+str(i)+path[dot_pos:]\n",
    "        with open(save_file_path,\"w+\",encoding=\"utf-8\") as fout:\n",
    "            fout.writelines(lines[(len(lines)*i)//10:(len(lines)*(i+1))//10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_text_to_file(text:str)->None:\n",
    "    with open(\"./output/log.txt\",\"a\") as fout:\n",
    "        fout.write(text+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from typing import *\n",
    "from predeal_dataset import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\JipingZhang\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertConfig,BertModel,BertForSequenceClassification,BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset,DataLoader,random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_EPOCH = 4\n",
    "BERT_BATCH_SIZE=5\n",
    "BERT_MAX_LEN = 384\n",
    "BERT_LR = 1e-6\n",
    "BERT_L2 = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_sentence_cnt = 0\n",
    "# with open(\"./goodreads_reviews_spoiler.json/goodreads_reviews_spoiler.json\",encoding=\"utf-8\") as fin:\n",
    "#     lines = fin.readlines()\n",
    "#     for line in tqdm(lines):\n",
    "#         line = line.replace(\"true\",\"True\")\n",
    "#         line = line.replace(\"false\",\"False\")\n",
    "#         datum = eval(line)\n",
    "#         for label,sentence in datum['review_sentences']:\n",
    "#             if len(sentence)>=VALID_SENTENCE_LEN_IN_CHAR_THS:\n",
    "#                 total_sentence_cnt+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split_raw_file(\"./goodreads_books.json/goodreads_books.json\")\n",
    "# split_raw_file(\"./goodreads_reviews_spoiler.json/goodreads_reviews_spoiler.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "spoiler_dataset = sample_sub_spoiler_set(SUBSET_SENTENCE_CNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_cnt = sum(d[\"label\"] for d in spoiler_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3330"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(spoiler_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function eval(source, globals=None, locals=None, /)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_correspondent_books_info(spoiler_dataset:List[dict],save_path:str)->List[dict]:\n",
    "    if os.path.exists(save_path):\n",
    "        res = list()\n",
    "        with open(save_path,encoding=\"utf-8\") as fin:\n",
    "            lines = fin.readlines()\n",
    "            for l in lines:\n",
    "                res.append(eval(l))\n",
    "        return res\n",
    "    books_required = set()\n",
    "    for d in spoiler_dataset:\n",
    "        books_required.add(d[\"book_id\"])\n",
    "    res = list()\n",
    "    for i in range(10):\n",
    "        with open(f\"./goodreads_books.json/goodreads_books{i}.json\",encoding=\"utf-8\") as fin:\n",
    "            lines = fin.readlines()\n",
    "            for line in tqdm(lines):\n",
    "                d = eval(line)\n",
    "                book_id = d[\"book_id\"]\n",
    "                if not book_id in books_required:\n",
    "                    continue\n",
    "                description = d[\"description\"]\n",
    "                title = d[\"title\"]\n",
    "                res.append({\"book_id\":book_id,\"description\":description,\"title\":title})\n",
    "        print(f\"altogether {len(res)} books has been found after searching {i+1} book info split files\")\n",
    "    try:\n",
    "        with open(save_path,\"w+\",encoding=\"utf-8\") as fout:\n",
    "            for datum in res:\n",
    "                fout.write(repr(datum)+\"\\n\")\n",
    "    except BaseException:\n",
    "        pass\n",
    "    return res            \n",
    "\n",
    "books_info = get_correspondent_books_info(spoiler_dataset,\"./sampled_datasets/s42_spoilers_correspondent_book.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16067"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books_required = set()\n",
    "for d in spoiler_dataset:\n",
    "    books_required.add(d[\"book_id\"])\n",
    "len(books_required)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:26<00:00, 188.78it/s]\n"
     ]
    }
   ],
   "source": [
    "statistic_arr = [0]*101\n",
    "for i in tqdm(range(len(spoiler_dataset[:len(spoiler_dataset)//10]))):\n",
    "    l = len(tokenizer(spoiler_dataset[i]['review_sentence'])['input_ids'])\n",
    "    if l>=1000:\n",
    "        l=1000\n",
    "    statistic_arr[l//10]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:00<00:00, 636600.19it/s]\n"
     ]
    }
   ],
   "source": [
    "statistic_arr2 = [0]*101\n",
    "for i in tqdm(range(len(spoiler_dataset[:len(spoiler_dataset)//10]))):\n",
    "    l = len(spoiler_dataset[i]['review_sentence'])\n",
    "    if l>=5000:\n",
    "        l=5000\n",
    "    statistic_arr2[l//50]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ListDataset(Dataset):\n",
    "#     def __init__(self,xs,ys) -> None:\n",
    "#         super().__init__()\n",
    "#         if len(xs)!=len(ys):\n",
    "#             raise ValueError\n",
    "#         self.xs = xs\n",
    "#         self.ys = ys\n",
    "    \n",
    "#     def __len__(self):\n",
    "#         return len(self.xs)\n",
    "    \n",
    "#     def __getitem__(self, index) -> Any:\n",
    "#         return self.xs[index],self.ys[index]\n",
    "    \n",
    "class ListDataset(Dataset):\n",
    "    def __init__(self,*lists) -> None:\n",
    "        super().__init__()\n",
    "        if len(lists)==0:\n",
    "            raise ValueError(\"Expecting at least one list\")\n",
    "        l = len(lists[0])\n",
    "        for i,li in enumerate(lists):\n",
    "            if not isinstance(li,(list,tuple,np.ndarray,torch.Tensor)):\n",
    "                raise ValueError(f\"expecting input to be list,tuple,numpy-array or torch's tensor, actually get {type(li)} at {i}-th argument\")\n",
    "            if len(li)!=l:\n",
    "                raise ValueError(f\"length of {i}-th argument is {len(li)}, length of 0-th argument is {l}, they don't match\")\n",
    "        self.lists = lists\n",
    "        self.l = l\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.l\n",
    "    \n",
    "    def __getitem__(self, index) -> Any:\n",
    "        return tuple(map(lambda l:l[index],self.lists))\n",
    "\n",
    "\n",
    "def get_spoiler_dataset(spoiler_dataset_raw:List[dict])->Dataset:\n",
    "    xs = list()\n",
    "    ys = list()\n",
    "    for datum in spoiler_dataset_raw:\n",
    "        xs.append(datum['review_sentence'])\n",
    "        ys.append(datum['label'])\n",
    "    return ListDataset(xs,ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "spoiler_dataset_processed = get_spoiler_dataset(spoiler_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_dataset, valid_dataset, test_dataset = random_split(spoiler_dataset_processed, [TRAIN_SET_CNT, VALID_SET_CNT, TEST_SET_CNT])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_device(d:dict,device=GPU)->dict:\n",
    "    for k in d:\n",
    "        d[k]=d[k].to(device)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "weights = [pos_cnt/SUBSET_SENTENCE_CNT,1-pos_cnt/SUBSET_SENTENCE_CNT]\n",
    "weights = torch.tensor(weights, dtype=torch.float).to(GPU)\n",
    "loss_func = torch.nn.CrossEntropyLoss(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_loader = DataLoader(train_dataset,batch_size=3,shuffle=True)\n",
    "# for batch in data_loader:\n",
    "#     print(batch)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_set_loader = DataLoader(valid_dataset,batch_size=BERT_BATCH_SIZE,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_performance_info(y_actual,y_predict):\n",
    "    y_actual = np.array(y_actual)\n",
    "    y_predict = np.array(y_predict)\n",
    "    y_actual = y_actual.reshape((-1,))\n",
    "    y_predict = y_predict.reshape((-1,))\n",
    "    TP = np.sum((y_actual == 1) & (y_predict == 1))\n",
    "    FP = np.sum((y_actual == 0) & (y_predict == 1))\n",
    "    TN = np.sum((y_actual == 0) & (y_predict == 0))\n",
    "    FN = np.sum((y_actual == 1) & (y_predict == 0))\n",
    "    TPR = TP / (TP + FN)\n",
    "    FPR = FP / (FP + TN)\n",
    "    TNR = TN / (TN + FP)\n",
    "    FNR = FN / (TP + FN)\n",
    "    BER = 1 - (0.5 * (TPR + TNR))\n",
    "    accu = np.sum(y_actual==y_predict)/len(y_actual)\n",
    "    return accu,TP,FP,TN,FN,TPR, FPR, TNR, FNR, BER\n",
    "\n",
    "def evaluate(model,dataloader):\n",
    "    y_pred = list()\n",
    "    y_label = list()\n",
    "    with torch.no_grad():\n",
    "        for b_x,b_y in tqdm(dataloader):\n",
    "            input_dict = tokenizer(b_x,padding=True,truncation=True,max_length=BERT_MAX_LEN,return_tensors=\"pt\")\n",
    "            output = (model(**to_device(input_dict,GPU)).logits).to(\"cpu\").detach()\n",
    "            y_label.extend(b_y.numpy().tolist())\n",
    "            y_pred.extend(torch.argmax(output,dim=1).numpy().tolist())\n",
    "    return get_performance_info(y_label,y_pred)\n",
    "\n",
    "e = 2.718281828\n",
    "\n",
    "def get_best_ths_with_ber(pred_prop_with_label): \n",
    "    pred_prop_with_label.sort(reverse=True)\n",
    "    valid_set_pos_cnt = sum(tup[1] for tup in pred_prop_with_label)\n",
    "    valid_set_neg_cnt = len(pred_prop_with_label)-valid_set_pos_cnt\n",
    "    best_ths = 1.0\n",
    "    best_ber = 0.5\n",
    "    curr_false_positive = 0\n",
    "    curr_false_negative = valid_set_pos_cnt\n",
    "    for (prob,label) in pred_prop_with_label:\n",
    "        ths = prob-0.00001\n",
    "        if label==1:\n",
    "            curr_false_negative-=1\n",
    "        else:\n",
    "            curr_false_positive+=1\n",
    "        ber = 0.5*(curr_false_negative/valid_set_pos_cnt+curr_false_positive/valid_set_neg_cnt)\n",
    "        if ber<best_ber:\n",
    "            best_ber = ber\n",
    "            best_ths = ths\n",
    "    return best_ths,best_ber\n",
    "\n",
    "def evaluate_dynamic_prob_ths(model,dataloader):\n",
    "    e = 2.718281828\n",
    "    y_pred_logits = list()\n",
    "    y_label = list()\n",
    "    with torch.no_grad():\n",
    "        for b_x,b_y in tqdm(dataloader):\n",
    "            input_dict = tokenizer(b_x,padding=True,truncation=True,max_length=BERT_MAX_LEN,return_tensors=\"pt\")\n",
    "            output = (model(**to_device(input_dict,GPU)).logits).to(\"cpu\").detach()\n",
    "            y_label.extend(b_y.numpy().tolist())\n",
    "            y_pred_logits.extend(output.numpy().tolist())\n",
    "    y_pos_prob_pred = list(map(lambda logits:e**logits[1]/(e**logits[0]+e**logits[1]),y_pred_logits))\n",
    "    pred_prob_with_label = list(zip(y_pos_prob_pred,y_label))\n",
    "    judging_ths,ber = get_best_ths_with_ber(pred_prob_with_label)\n",
    "    y_pred = list(int(p>judging_ths) for p in y_pos_prob_pred)\n",
    "    return *get_performance_info(y_label,y_pred),judging_ths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate_dynamic_prob_ths(model,valid_set_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  7%|▋         | 590/8000 [03:27<42:11,  2.93it/s] "
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(model_type)\n",
    "model.to(GPU)\n",
    "optimizer = torch.optim.Adam(model.parameters(),BERT_LR,weight_decay=BERT_L2)\n",
    "optimizer_warmup = torch.optim.Adam(model.classifier.parameters(),0.01,weight_decay=0.01)\n",
    "\n",
    "data_loader = DataLoader(train_dataset,batch_size=BERT_BATCH_SIZE,shuffle=True)\n",
    "\n",
    "log_text_to_file(\"start training BertForSequenceClassification model , dataset is review_text only\")\n",
    "for e in range(BERT_EPOCH):\n",
    "    for step,(b_x,b_y) in enumerate(tqdm(data_loader)):\n",
    "        input_dict = tokenizer(b_x,padding=True,truncation=True,max_length=BERT_MAX_LEN,return_tensors=\"pt\")\n",
    "        output = model(**to_device(input_dict,GPU)).logits\n",
    "        # print(output)\n",
    "        b_y = b_y.to(GPU)\n",
    "        loss = loss_func(output,b_y)\n",
    "        if step>=100 or e>=1:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        else:\n",
    "            optimizer_warmup.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer_warmup.step()\n",
    "    accu,TP,FP,TN,FN,TPR, FPR, TNR, FNR, BER,ths = evaluate_dynamic_prob_ths(model,valid_set_loader)\n",
    "    print(f\"ber after epoch {e+1}: {BER}\")\n",
    "    msg = \"    %7s%7s%7s%7s%7s%7s%7s\\n     %.4f %.4f %.4f %.4f %.4f %.4f %.4f \"%(\"accu\",\"ber\",\"tpr\",\"fpr\",\"tnr\",\"fnr\",\"ths\",accu,BER,TPR,FPR,TNR,FNR,ths)\n",
    "    # print(accu,TP,FP,TN,FN,TPR, FPR, TNR, FNR, BER)\n",
    "    print(msg)\n",
    "    save_path = f\"./output/review_only_bert_e{e}_b{BERT_BATCH_SIZE}_lr{BERT_LR}_l2{BERT_L2}_ber{BER:.4f}\"\n",
    "    torch.save(model.state_dict(),save_path)\n",
    "    log_text_to_file(f\"    ber after epoch {e+1}: {BER}, model params saved to {save_path}, other performance infos:{accu,TP,FP,TN,FN,TPR, FPR, TNR, FNR, BER}\")\n",
    "    log_text_to_file(msg)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = \"./output/review_only_bert_e1_b5_lr1e-06_l20.0001_ber0.3214\"\n",
    "model = BertForSequenceClassification.from_pretrained(model_type)\n",
    "model.load_state_dict(torch.load(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [02:00<00:00,  8.30it/s]\n"
     ]
    }
   ],
   "source": [
    "y_label = list()\n",
    "y_logits = list()\n",
    "with torch.no_grad():\n",
    "    for b_x,b_y in tqdm(DataLoader(valid_dataset,batch_size=BERT_BATCH_SIZE,shuffle=False)):\n",
    "        input_dict = tokenizer(b_x,padding=True,truncation=True,max_length=BERT_MAX_LEN,return_tensors=\"pt\")\n",
    "        output = model(**to_device(input_dict,GPU)).logits\n",
    "        y_label.extend(b_y.numpy().tolist())\n",
    "        y_logits.extend(output.to(\"cpu\").detach().numpy().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = 2.718281828\n",
    "pred_prop_with_label = [(e**logit_pos/(e**logit_neg+e**logit_pos),lbl) for (logit_neg,logit_pos),lbl in zip(y_logits,y_label)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_prop_with_label.sort(reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_set_pos_cnt = sum(tup[1] for tup in valid_dataset)\n",
    "valid_set_neg_cnt = VALID_SET_CNT - valid_set_pos_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.16590920771243706, 0.23056044209890364)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_best_ths_with_ber(pred_prop_with_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.4346359008445251, 0),\n",
       " (0.43437236011218483, 0),\n",
       " (0.4343054224677632, 1),\n",
       " (0.43396178141716973, 1),\n",
       " (0.433760607269941, 1),\n",
       " (0.43355051661465943, 0),\n",
       " (0.43293000668913073, 0),\n",
       " (0.4327360277277486, 0),\n",
       " (0.43263807166233326, 0),\n",
       " (0.43262990135057205, 0),\n",
       " (0.4323402543622263, 0),\n",
       " (0.4322489788072485, 0),\n",
       " (0.43189199179049115, 0),\n",
       " (0.4311957995221293, 0),\n",
       " (0.43095367614642016, 0),\n",
       " (0.43048090113385756, 0),\n",
       " (0.42984434749414746, 0),\n",
       " (0.42977580631168005, 0),\n",
       " (0.4295807477828543, 0),\n",
       " (0.4292543736454375, 0)]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_prop_with_label[520:540]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_sentence_puncts = [ch for ch in string.punctuation if ch not in \"'()[]{}\"]\n",
    "def cut_sub_sentences(sentence:str)->List[str]:\n",
    "    sentence = sentence.replace(\"\\n\",\" \").replace(\"\\t\",\" \")\n",
    "    res:List[str] = list()\n",
    "    res.append(sentence)\n",
    "    for pun in cut_sentence_puncts:\n",
    "        res_new = list()\n",
    "        for sub_sen in res:\n",
    "            res_new.extend(sub_sen.split(pun))\n",
    "        res = res_new\n",
    "    res = list(filter(lambda string:len(string)>0,map(lambda x:x.strip(),res)))\n",
    "    return res\n",
    "\n",
    "def fetch_longest_k_sub_sentences(sentence:str,k:int=1)->List[str]:\n",
    "    sentences = cut_sub_sentences(sentence)\n",
    "    if len(sentences)<=k:\n",
    "        return sentences\n",
    "    sentences = list(enumerate(sentences))\n",
    "    sentences.sort(key=lambda tup:len(tup[1]),reverse=True)\n",
    "    res = sentences[:k]\n",
    "    res.sort()\n",
    "    return list(map(lambda tup:tup[1],res))\n",
    "\n",
    "def connect_title_and_desc(title:str,description:str,k:int=1,*args,**kwargs)->str:\n",
    "    res = title.strip()\n",
    "    if len(res)==0:\n",
    "        return \"\"\n",
    "    if not res[-1] in cut_sentence_puncts:\n",
    "        res+=\".\"\n",
    "    if k==0:\n",
    "        return res\n",
    "    if description is None:\n",
    "        return res\n",
    "    if len(description)==0:\n",
    "        return res\n",
    "    res+=\" \"\n",
    "    longest_sub_sen = fetch_longest_k_sub_sentences(description,k)\n",
    "    for desc_sub_sen in longest_sub_sen[:-1]:\n",
    "        res+=desc_sub_sen+\", \"\n",
    "    res+=longest_sub_sen[-1]+\".\"\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\JipingZhang\\Desktop\\cse-258-rec-sys\\assignment2\\book review\\play_bert.ipynb Cell 43\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/JipingZhang/Desktop/cse-258-rec-sys/assignment2/book%20review/play_bert.ipynb#Y103sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# cut_sub_sentences(books_info[0][\"description\"])\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/JipingZhang/Desktop/cse-258-rec-sys/assignment2/book%20review/play_bert.ipynb#Y103sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m connect_title_and_desc(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mbooks_info[\u001b[39m7\u001b[39m])\n",
      "\u001b[1;32mc:\\Users\\JipingZhang\\Desktop\\cse-258-rec-sys\\assignment2\\book review\\play_bert.ipynb Cell 43\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/JipingZhang/Desktop/cse-258-rec-sys/assignment2/book%20review/play_bert.ipynb#Y103sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39mfor\u001b[39;00m desc_sub_sen \u001b[39min\u001b[39;00m longest_sub_sen[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/JipingZhang/Desktop/cse-258-rec-sys/assignment2/book%20review/play_bert.ipynb#Y103sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m     res\u001b[39m+\u001b[39m\u001b[39m=\u001b[39mdesc_sub_sen\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/JipingZhang/Desktop/cse-258-rec-sys/assignment2/book%20review/play_bert.ipynb#Y103sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m res\u001b[39m+\u001b[39m\u001b[39m=\u001b[39mlongest_sub_sen[\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m]\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/JipingZhang/Desktop/cse-258-rec-sys/assignment2/book%20review/play_bert.ipynb#Y103sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m \u001b[39mreturn\u001b[39;00m res\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# cut_sub_sentences(books_info[0][\"description\"])\n",
    "connect_title_and_desc(**books_info[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16067/16067 [00:01<00:00, 11321.13it/s]\n"
     ]
    }
   ],
   "source": [
    "book_info_dict:Dict[str,str] = dict()\n",
    "for book_info in tqdm(books_info):\n",
    "    book_info_dict[book_info['book_id']]=connect_title_and_desc(**book_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': 0,\n",
       " 'book_id': '24473763',\n",
       " 'rating': 4,\n",
       " 'review_sentence': 'Can we please have more nice guys in romance fantasy fiction? I am so sick and tired of Alpha assholes. Seriously, they are a grade A yawn and I\\'m not frankly into the love turn hate trope. Some books do it well but for the most part, the majority of them are cringeworthy where the Hero should be reported for Abuse rather than be swoon over. So when a book touts a nice respectful guy as the hero, I\\'m all ears. And this book DELIVERS. It effing delivers a swoon-worthy believable romance with a nice guy who RESPECTS and CHERISHES his woman. Plus he\\'s also hot when he get mad at other people for stepping on his woman. The romance was written so well and brings to life the phrase \"falling in love\". Yeah, I\\'m into that. Gimme more. It does have it flaws like the plot is basically non-existent but who cares, it\\'s really about these two. Can\\'t wait for book 2!!!! '}"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spoiler_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spoiler_dataset_with_book_desc(spoiler_dataset_raw:List[dict],seperator:str=\"[SEP]\")->Dataset:\n",
    "    xs = list()\n",
    "    ys = list()\n",
    "    for datum in spoiler_dataset_raw:\n",
    "        xs.append(f\"{book_info_dict[datum['book_id']]} {seperator} {datum['review_sentence']}\")\n",
    "        ys.append(datum['label'])\n",
    "    return ListDataset(xs,ys)\n",
    "\n",
    "spoiler_dataset_processed_with_book_desc = get_spoiler_dataset_with_book_desc(spoiler_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_wd, valid_dataset_wd, test_dataset_wd = random_split(spoiler_dataset_processed, [TRAIN_SET_CNT, VALID_SET_CNT, TEST_SET_CNT])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_set_loader_wd = DataLoader(valid_dataset_wd,batch_size=BERT_BATCH_SIZE,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8000/8000 [38:04<00:00,  3.50it/s]\n",
      "100%|██████████| 1000/1000 [01:46<00:00,  9.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ber after epoch 2: 0.22667584196392432\n",
      "       accu    ber    tpr    fpr    tnr    fnr    ths\n",
      "    0.7484 0.2267 0.8023 0.2556 0.7444 0.1977 0.2148 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8000/8000 [38:02<00:00,  3.51it/s]\n",
      "100%|██████████| 1000/1000 [01:45<00:00,  9.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ber after epoch 3: 0.22890046137288156\n",
      "       accu    ber    tpr    fpr    tnr    fnr    ths\n",
      "    0.7344 0.2289 0.8138 0.2716 0.7284 0.1862 0.1416 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 596/8000 [02:52<35:37,  3.46it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\JipingZhang\\Desktop\\cse-258-rec-sys\\assignment2\\book review\\play_bert.ipynb Cell 49\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/JipingZhang/Desktop/cse-258-rec-sys/assignment2/book%20review/play_bert.ipynb#Y113sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m output \u001b[39m=\u001b[39m model_wd(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mto_device(input_dict,GPU))\u001b[39m.\u001b[39mlogits\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/JipingZhang/Desktop/cse-258-rec-sys/assignment2/book%20review/play_bert.ipynb#Y113sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m# print(output)\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/JipingZhang/Desktop/cse-258-rec-sys/assignment2/book%20review/play_bert.ipynb#Y113sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m b_y \u001b[39m=\u001b[39m b_y\u001b[39m.\u001b[39;49mto(GPU)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/JipingZhang/Desktop/cse-258-rec-sys/assignment2/book%20review/play_bert.ipynb#Y113sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m loss \u001b[39m=\u001b[39m loss_func(output,b_y)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/JipingZhang/Desktop/cse-258-rec-sys/assignment2/book%20review/play_bert.ipynb#Y113sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mif\u001b[39;00m step\u001b[39m>\u001b[39m\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m \u001b[39mor\u001b[39;00m e\u001b[39m>\u001b[39m\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_wd = BertForSequenceClassification.from_pretrained(model_type)\n",
    "model_wd.to(GPU)\n",
    "optimizer = torch.optim.Adam(model_wd.parameters(),BERT_LR,weight_decay=BERT_L2)\n",
    "optimizer_warmup = torch.optim.Adam(model_wd.classifier.parameters(),0.01,weight_decay=0.01)\n",
    "\n",
    "data_loader_wd = DataLoader(train_dataset_wd,batch_size=BERT_BATCH_SIZE,shuffle=True)\n",
    "\n",
    "log_text_to_file(\"start training BertForSequenceClassification model , dataset is review_text_wd\")\n",
    "for e in range(1,BERT_EPOCH):\n",
    "    for step,(b_x,b_y) in enumerate(tqdm(data_loader_wd)):\n",
    "        input_dict = tokenizer(b_x,padding=True,truncation=True,max_length=BERT_MAX_LEN,return_tensors=\"pt\")\n",
    "        output = model_wd(**to_device(input_dict,GPU)).logits\n",
    "        # print(output)\n",
    "        b_y = b_y.to(GPU)\n",
    "        loss = loss_func(output,b_y)\n",
    "        if step>=100 or e>=1:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        else:\n",
    "            optimizer_warmup.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer_warmup.step()\n",
    "    accu,TP,FP,TN,FN,TPR, FPR, TNR, FNR, BER,ths = evaluate_dynamic_prob_ths(model_wd,valid_set_loader_wd)\n",
    "    print(f\"ber after epoch {e+1}: {BER}\")\n",
    "    msg = \"    %7s%7s%7s%7s%7s%7s%7s\\n     %.4f %.4f %.4f %.4f %.4f %.4f %.4f \"%(\"accu\",\"ber\",\"tpr\",\"fpr\",\"tnr\",\"fnr\",\"ths\",accu,BER,TPR,FPR,TNR,FNR,ths)\n",
    "    # print(accu,TP,FP,TN,FN,TPR, FPR, TNR, FNR, BER)\n",
    "    print(msg)\n",
    "    save_path = f\"./output/review_wd_bert_e{e}_b{BERT_BATCH_SIZE}_lr{BERT_LR}_l2{BERT_L2}_ber{BER:.4f}\"\n",
    "    torch.save(model_wd.state_dict(),save_path)\n",
    "    log_text_to_file(f\"    ber after epoch {e+1}: {BER}, model params saved to {save_path}, other performance infos:{accu,TP,FP,TN,FN,TPR, FPR, TNR, FNR, BER}\")\n",
    "    log_text_to_file(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:45<00:00,  9.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ber after epoch 3.718281828: 0.24025427566182578\n",
      "       accu    ber    tpr    fpr    tnr    fnr    ths\n",
      "    0.733000.240250.790830.271340.728660.209170.28194\n"
     ]
    }
   ],
   "source": [
    "# accu,TP,FP,TN,FN,TPR, FPR, TNR, FNR, BER,ths = evaluate_dynamic_prob_ths(model_wd,valid_set_loader_wd)\n",
    "# print(f\"ber after epoch {e+1}: {BER}\")\n",
    "# msg = \"    %7s%7s%7s%7s%7s%7s%7s\\n    %.4f %.4f %.4f %.4f %.4f %.4f %.4f \"%(\"accu\",\"ber\",\"tpr\",\"fpr\",\"tnr\",\"fnr\",\"ths\",accu,BER,TPR,FPR,TNR,FNR,ths)\n",
    "#     # print(accu,TP,FP,TN,FN,TPR, FPR, TNR, FNR, BER)\n",
    "# print(msg)\n",
    "# save_path = f\"./output/review_wd_bert_e{e}_b{BERT_BATCH_SIZE}_lr{BERT_LR}_l2{BERT_L2}_ber{BER:.4f}\"\n",
    "# torch.save(model_wd.state_dict(),save_path)\n",
    "# log_text_to_file(f\"    ber after epoch {e+1}: {BER}, model params saved to {save_path}, other performance infos:{accu,TP,FP,TN,FN,TPR, FPR, TNR, FNR, BER}\")\n",
    "# log_text_to_file(msg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
