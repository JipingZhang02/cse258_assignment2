{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from typing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\JipingZhang\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertConfig,BertModel,BertForSequenceClassification,BertTokenizer\n",
    "GPU = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset,DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBSET_SENTENCE_CNT = 100000\n",
    "VALID_SENTENCE_LEN_IN_CHAR_THS = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_sub_spoiler_set(sentence_cnt:int,seed:int=42)->List[dict]:\n",
    "    random.seed(seed)\n",
    "    path = f\"./sampled_datasets/review_spoiler_dataset_l{sentence_cnt}_s{seed}.txt\"\n",
    "    if os.path.exists(path):\n",
    "        res = list()\n",
    "        with open(path) as fin:\n",
    "            lines = fin.readlines()\n",
    "            for l in lines:\n",
    "                res.append(eval(l))\n",
    "        return res\n",
    "    random.seed(seed)\n",
    "    res = list()\n",
    "    viewed_sample_cnt=0\n",
    "    with open(\"./goodreads_reviews_spoiler.json/goodreads_reviews_spoiler.json\",encoding=\"utf-8\") as fin:\n",
    "        lines = fin.readlines()\n",
    "        for line in tqdm(lines):\n",
    "            line = line.replace(\"true\",\"True\")\n",
    "            line = line.replace(\"false\",\"False\")\n",
    "            datum = eval(line)\n",
    "            book_id = datum['book_id']\n",
    "            rating = datum['rating']\n",
    "            for label,sentence in datum['review_sentences']:\n",
    "                if len(sentence)<VALID_SENTENCE_LEN_IN_CHAR_THS:\n",
    "                    continue\n",
    "                viewed_sample_cnt+=1\n",
    "                if len(res)<sentence_cnt:\n",
    "                    d = dict()\n",
    "                    d[\"label\"]=label\n",
    "                    d[\"review_sentence\"]=sentence\n",
    "                    d[\"book_id\"]=book_id\n",
    "                    d['rating']=rating\n",
    "                    res.append(d)\n",
    "                else:\n",
    "                    i = random.randint(0,viewed_sample_cnt-1)\n",
    "                    if i<sentence_cnt:\n",
    "                        d = dict()\n",
    "                        d[\"label\"]=label\n",
    "                        d[\"review_sentence\"]=sentence\n",
    "                        d[\"book_id\"]=book_id\n",
    "                        d['rating']=rating\n",
    "                        res[i]=d\n",
    "    with open(path,\"w+\") as fout:\n",
    "        for datum in res:\n",
    "            fout.write(repr(datum)+\"\\n\")\n",
    "    return res\n",
    "spoiler_dataset = sample_sub_spoiler_set(SUBSET_SENTENCE_CNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': 1,\n",
       " 'review_sentence': 'The magus said, \"I think if you took the time to look, you might see that over the space of a year you turned into the greatest folk hero Eddis has ever known.\"',\n",
       " 'book_id': '40158',\n",
       " 'rating': 5}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spoiler_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_type = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_type)\n",
    "model = BertForSequenceClassification.from_pretrained(model_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[-0.0940, -0.3300],\n",
       "        [-0.1193, -0.5823]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = [\"I love you. Springboot is a java web framework.\",\"What is this?\"]\n",
    "input_dict = tokenizer(sentences,padding=True,truncation=True,max_length=256,return_tensors=\"pt\")\n",
    "model(**input_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ListDataset(Dataset):\n",
    "    def __init__(self,xs,ys) -> None:\n",
    "        super().__init__()\n",
    "        if len(xs)!=len(ys):\n",
    "            raise ValueError\n",
    "        self.xs = xs\n",
    "        self.ys = ys\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.xs)\n",
    "    \n",
    "    def __getitem__(self, index) -> Any:\n",
    "        return self.xs[index],self.ys[index]\n",
    "\n",
    "def get_spoiler_dataset(spoiler_dataset_raw:List[dict])->Dataset:\n",
    "    xs = list()\n",
    "    ys = list()\n",
    "    for datum in spoiler_dataset_raw:\n",
    "        xs.append(datum['review_sentence'])\n",
    "        ys.append(datum['label'])\n",
    "    return ListDataset(xs,ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "spoiler_dataset_processed = get_spoiler_dataset(spoiler_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('And it\\'s taking us out of the world.\"', 'I wish there were in AI character.') tensor([0, 0])\n"
     ]
    }
   ],
   "source": [
    "data_loader = DataLoader(spoiler_dataset_processed,batch_size=2,shuffle=True)\n",
    "\n",
    "for b_x,b_y in data_loader:\n",
    "    print(b_x,b_y)\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
